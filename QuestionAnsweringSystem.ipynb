{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "71cc090d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Who directed the movie 'Titanic'?</td>\n",
       "      <td>JamesCameron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Which superhero is also known as the Dark Knight?</td>\n",
       "      <td>Batman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>What is the capital of Brazil?</td>\n",
       "      <td>Brasilia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Which fruit is known as the king of fruits?</td>\n",
       "      <td>Mango</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Which country is known for the Eiffel Tower?</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question        answer\n",
       "0                      What is the capital of France?         Paris\n",
       "1                     What is the capital of Germany?        Berlin\n",
       "2                  Who wrote 'To Kill a Mockingbird'?    Harper-Lee\n",
       "3     What is the largest planet in our solar system?       Jupiter\n",
       "4      What is the boiling point of water in Celsius?           100\n",
       "..                                                ...           ...\n",
       "85                  Who directed the movie 'Titanic'?  JamesCameron\n",
       "86  Which superhero is also known as the Dark Knight?        Batman\n",
       "87                     What is the capital of Brazil?      Brasilia\n",
       "88        Which fruit is known as the king of fruits?         Mango\n",
       "89       Which country is known for the Eiffel Tower?        France\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"100_Unique_QA_Dataset.csv\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "48584045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'the', 'capital', 'of', 'france', 'paris']\n",
      "['what', 'is', 'the', 'capital', 'of', 'germany', 'berlin']\n",
      "['who', 'wrote', 'to', 'kill', 'a', 'mockingbird', 'harper-lee']\n",
      "['what', 'is', 'the', 'largest', 'planet', 'in', 'our', 'solar', 'system', 'jupiter']\n",
      "['what', 'is', 'the', 'boiling', 'point', 'of', 'water', 'in', 'celsius', '100']\n",
      "['who', 'painted', 'the', 'mona', 'lisa', 'leonardo-da-vinci']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '64', '8']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'gold', 'au']\n",
      "['which', 'year', 'did', 'world', 'war', 'ii', 'end', '1945']\n",
      "['what', 'is', 'the', 'longest', 'river', 'in', 'the', 'world', 'nile']\n",
      "['what', 'is', 'the', 'capital', 'of', 'japan', 'tokyo']\n",
      "['who', 'developed', 'the', 'theory', 'of', 'relativity', 'albert-einstein']\n",
      "['what', 'is', 'the', 'freezing', 'point', 'of', 'water', 'in', 'fahrenheit', '32']\n",
      "['which', 'planet', 'is', 'known', 'as', 'the', 'red', 'planet', 'mars']\n",
      "['who', 'is', 'the', 'author', 'of', '1984', 'george-orwell']\n",
      "['what', 'is', 'the', 'currency', 'of', 'the', 'united', 'kingdom', 'pound']\n",
      "['what', 'is', 'the', 'capital', 'of', 'india', 'delhi']\n",
      "['who', 'discovered', 'gravity', 'newton']\n",
      "['how', 'many', 'continents', 'are', 'there', 'on', 'earth', '7']\n",
      "['which', 'gas', 'do', 'plants', 'use', 'for', 'photosynthesis', 'co2']\n",
      "['what', 'is', 'the', 'smallest', 'prime', 'number', '2']\n",
      "['who', 'invented', 'the', 'telephone', 'alexander-graham-bell']\n",
      "['what', 'is', 'the', 'capital', 'of', 'australia', 'canberra']\n",
      "['which', 'ocean', 'is', 'the', 'largest', 'pacific-ocean']\n",
      "['what', 'is', 'the', 'speed', 'of', 'light', 'in', 'vacuum', '299,792,458m/s']\n",
      "['which', 'language', 'is', 'spoken', 'in', 'brazil', 'portuguese']\n",
      "['who', 'discovered', 'penicillin', 'alexander-fleming']\n",
      "['what', 'is', 'the', 'capital', 'of', 'canada', 'ottawa']\n",
      "['what', 'is', 'the', 'largest', 'mammal', 'on', 'earth', 'whale']\n",
      "['which', 'element', 'has', 'the', 'atomic', 'number', '1', 'hydrogen']\n",
      "['what', 'is', 'the', 'tallest', 'mountain', 'in', 'the', 'world', 'everest']\n",
      "['which', 'city', 'is', 'known', 'as', 'the', 'big', 'apple', 'newyork']\n",
      "['how', 'many', 'planets', 'are', 'in', 'the', 'solar', 'system', '8']\n",
      "['who', 'painted', 'starry', 'night', 'vangogh']\n",
      "['what', 'is', 'the', 'chemical', 'formula', 'of', 'water', 'h2o']\n",
      "['what', 'is', 'the', 'capital', 'of', 'italy', 'rome']\n",
      "['which', 'country', 'is', 'famous', 'for', 'sushi', 'japan']\n",
      "['who', 'was', 'the', 'first', 'person', 'to', 'step', 'on', 'the', 'moon', 'armstrong']\n",
      "['what', 'is', 'the', 'main', 'ingredient', 'in', 'guacamole', 'avocado']\n",
      "['how', 'many', 'sides', 'does', 'a', 'hexagon', 'have', '6']\n",
      "['what', 'is', 'the', 'currency', 'of', 'china', 'yuan']\n",
      "['who', 'wrote', 'pride', 'and', 'prejudice', 'jane-austen']\n",
      "['what', 'is', 'the', 'chemical', 'symbol', 'for', 'iron', 'fe']\n",
      "['what', 'is', 'the', 'hardest', 'natural', 'substance', 'on', 'earth', 'diamond']\n",
      "['which', 'continent', 'is', 'the', 'largest', 'by', 'area', 'asia']\n",
      "['who', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states', 'george-washington']\n",
      "['which', 'bird', 'is', 'known', 'for', 'its', 'ability', 'to', 'mimic', 'sounds', 'parrot']\n",
      "['what', 'is', 'the', 'longest-running', 'animated', 'tv', 'show', 'simpsons']\n",
      "['what', 'is', 'the', 'smallest', 'country', 'in', 'the', 'world', 'vaticancity']\n",
      "['which', 'planet', 'has', 'the', 'most', 'moons', 'saturn']\n",
      "['who', 'wrote', 'romeo', 'and', 'juliet', 'shakespeare']\n",
      "['what', 'is', 'the', 'main', 'gas', 'in', 'earths', 'atmosphere', 'nitrogen']\n",
      "['how', 'many', 'bones', 'are', 'in', 'the', 'adult', 'human', 'body', '206']\n",
      "['which', 'metal', 'is', 'a', 'liquid', 'at', 'room', 'temperature', 'mercury']\n",
      "['what', 'is', 'the', 'capital', 'of', 'russia', 'moscow']\n",
      "['who', 'discovered', 'electricity', 'benjamin-franklin']\n",
      "['which', 'is', 'the', 'second-largest', 'country', 'by', 'land', 'area', 'canada']\n",
      "['what', 'is', 'the', 'color', 'of', 'a', 'ripe', 'banana', 'yellow']\n",
      "['which', 'month', 'has', '28', 'days', 'in', 'a', 'common', 'year', 'february']\n",
      "['what', 'is', 'the', 'study', 'of', 'living', 'organisms', 'called', 'biology']\n",
      "['which', 'country', 'is', 'home', 'to', 'the', 'great', 'wall', 'china']\n",
      "['what', 'do', 'bees', 'collect', 'from', 'flowers', 'nectar']\n",
      "['what', 'is', 'the', 'opposite', 'of', 'day', 'night']\n",
      "['what', 'is', 'the', 'capital', 'of', 'south', 'korea', 'seoul']\n",
      "['who', 'invented', 'the', 'light', 'bulb', 'edison']\n",
      "['which', 'gas', 'do', 'humans', 'breathe', 'in', 'for', 'survival', 'oxygen']\n",
      "['what', 'is', 'the', 'square', 'root', 'of', '144', '12']\n",
      "['which', 'country', 'has', 'the', 'pyramids', 'of', 'giza', 'egypt']\n",
      "['which', 'sea', 'creature', 'has', 'eight', 'arms', 'octopus']\n",
      "['which', 'holiday', 'is', 'celebrated', 'on', 'december', '25', 'christmas']\n",
      "['what', 'is', 'the', 'currency', 'of', 'japan', 'yen']\n",
      "['how', 'many', 'legs', 'does', 'a', 'spider', 'have', '8']\n",
      "['which', 'sport', 'uses', 'a', 'net,', 'ball,', 'and', 'hoop', 'basketball']\n",
      "['which', 'country', 'is', 'famous', 'for', 'its', 'kangaroos', 'australia']\n",
      "['who', 'was', 'the', 'first', 'female', 'prime', 'minister', 'of', 'the', 'uk', 'margaretthatcher']\n",
      "['which', 'is', 'the', 'fastest', 'land', 'animal', 'cheetah']\n",
      "['what', 'is', 'the', 'first', 'element', 'on', 'the', 'periodic', 'table', 'hydrogen']\n",
      "['what', 'is', 'the', 'capital', 'of', 'spain', 'madrid']\n",
      "['which', 'planet', 'is', 'the', 'closest', 'to', 'the', 'sun', 'mercury']\n",
      "['who', 'is', 'known', 'as', 'the', 'father', 'of', 'computers', 'charlesbabbage']\n",
      "['what', 'is', 'the', 'capital', 'of', 'mexico', 'mexicocity']\n",
      "['how', 'many', 'colors', 'are', 'in', 'a', 'rainbow', '7']\n",
      "['which', 'musical', 'instrument', 'has', 'black', 'and', 'white', 'keys', 'piano']\n",
      "['who', 'discovered', 'the', 'americas', 'in', '1492', 'christophercolumbus']\n",
      "['which', 'disney', 'character', 'has', 'a', 'long', 'nose', 'and', 'grows', 'it', 'when', 'lying', 'pinocchio']\n",
      "['who', 'directed', 'the', 'movie', 'titanic', 'jamescameron']\n",
      "['which', 'superhero', 'is', 'also', 'known', 'as', 'the', 'dark', 'knight', 'batman']\n",
      "['what', 'is', 'the', 'capital', 'of', 'brazil', 'brasilia']\n",
      "['which', 'fruit', 'is', 'known', 'as', 'the', 'king', 'of', 'fruits', 'mango']\n",
      "['which', 'country', 'is', 'known', 'for', 'the', 'eiffel', 'tower', 'france']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "def tokenization(text):\n",
    "    text=text.lower()\n",
    "    text=text.replace(\"?\",\"\")\n",
    "    text=text.replace(\"'\",\"\")\n",
    "    return text.split()\n",
    "#vocab\n",
    "vocab={'<UNK>':0}\n",
    "def build_vocab(row):\n",
    "    tokenized_question=tokenization(row['question'])\n",
    "    tokenized_answer=tokenization(row['answer'])\n",
    "\n",
    "    merged_token=tokenized_question+tokenized_answer\n",
    "    for token in merged_token:\n",
    "        if token not in vocab:\n",
    "            vocab[token]=len(vocab)\n",
    "    print(merged_token)\n",
    "df.apply(build_vocab,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d9af27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert words to numerical indices\n",
    "def text_to_indices(text,vocab):\n",
    "    indexed_text=[]\n",
    "    for token in tokenization(text):\n",
    "        if token in vocab:\n",
    "            indexed_text.append(vocab[token])\n",
    "        else:\n",
    "            indexed_text.append(vocab['<UNK>'])\n",
    "    return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3a13b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "112f09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAdataset(Dataset):\n",
    "    def __init__(self,df,vocab):\n",
    "        self.df=df\n",
    "        self.vocab=vocab\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    def __getitem__(self,index):\n",
    "        numerical_question=text_to_indices(self.df.iloc[index]['question'],self.vocab)\n",
    "        numerical_answer=text_to_indices(self.df.iloc[index]['answer'],self.vocab)\n",
    "\n",
    "        return torch.tensor(numerical_question),torch.tensor(numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43efd01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds=QAdataset(df,vocab)\n",
    "dataloader=DataLoader(ds,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f853a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([16])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([246])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([188])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([91])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([128])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([131])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([68])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([124])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([53])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([249])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([321])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([273])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([9])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([121])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([100])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([194])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([58])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([162])\n",
      "tensor([[10, 96,  3, 97]]) tensor([98])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([102])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([155])\n",
      "tensor([[ 10,  75, 208]]) tensor([209])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([244])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([233])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([61])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([154])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([311])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([74])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([268])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([295])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([110])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([276])\n",
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([185])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([280])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([41])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([136])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([191])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([160])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([149])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([285])\n",
      "tensor([[10, 75, 76]]) tensor([77])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([199])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([95])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([132])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([259])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([179])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([225])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([317])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([114])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([36])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([23])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([220])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([156])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([85])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([72])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([54])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([113])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([85])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([166])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([254])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([65])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([6])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([32])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([298])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([170])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([287])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([173])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([121])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([207])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([99])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([36])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([145])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([260])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([134])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([238])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([106])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([205])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([28])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([205])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([184])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([240])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([36])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([7])\n",
      "tensor([[ 10,  75, 111]]) tensor([112])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([49])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([52])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([215])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([307])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([316])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([116])\n"
     ]
    }
   ],
   "source": [
    "for que,ans in dataloader:\n",
    "    print(que,ans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "88a77866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8662f257",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim=50)\n",
    "        self.rnn=nn.RNN(50,64,batch_first=True)\n",
    "        self.fc=nn.Linear(64,vocab_size)\n",
    "\n",
    "    def forward(self,que):\n",
    "        embedded_question=self.embedding(que)\n",
    "        hidden,final=self.rnn(embedded_question)\n",
    "        output=self.fc(final.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6acdcb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "epochs=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89784480",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bc3996db",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14f90feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1, Loss 537.5186033248901\n",
      "Epochs 2, Loss 318.5814647078514\n",
      "Epochs 3, Loss 145.1672183573246\n",
      "Epochs 4, Loss 72.75360015314072\n",
      "Epochs 5, Loss 30.282578042708337\n",
      "Epochs 6, Loss 34.658514665439725\n",
      "Epochs 7, Loss 27.32634341204539\n",
      "Epochs 8, Loss 25.37221501278691\n",
      "Epochs 9, Loss 22.72959983209148\n",
      "Epochs 10, Loss 12.712916251271963\n",
      "Epochs 11, Loss 11.264159947866574\n",
      "Epochs 12, Loss 17.20179022871889\n",
      "Epochs 13, Loss 14.901815988821909\n",
      "Epochs 14, Loss 14.542612265329808\n",
      "Epochs 15, Loss 26.707070981385186\n",
      "Epochs 16, Loss 19.35438282089308\n",
      "Epochs 17, Loss 13.750045066786697\n",
      "Epochs 18, Loss 11.03483790656901\n",
      "Epochs 19, Loss 15.09694543943624\n",
      "Epochs 20, Loss 13.173345675546443\n",
      "Epochs 21, Loss 5.6671391656273045\n",
      "Epochs 22, Loss 7.7220409766887315\n",
      "Epochs 23, Loss 1.770843015779974\n",
      "Epochs 24, Loss 3.2108779834525194\n",
      "Epochs 25, Loss 10.172223000088707\n",
      "Epochs 26, Loss 8.744661755568814\n",
      "Epochs 27, Loss 6.42975227496936\n",
      "Epochs 28, Loss 3.129518974048551\n",
      "Epochs 29, Loss 2.3823428476171102\n",
      "Epochs 30, Loss 0.3583002353552729\n",
      "Epochs 31, Loss 0.234335316112265\n",
      "Epochs 32, Loss 0.20460997548070736\n",
      "Epochs 33, Loss 0.18438379297731444\n",
      "Epochs 34, Loss 0.16777690379240084\n",
      "Epochs 35, Loss 0.15380722704867367\n",
      "Epochs 36, Loss 0.14323516323929653\n",
      "Epochs 37, Loss 0.13313761884637643\n",
      "Epochs 38, Loss 0.1250070321984822\n",
      "Epochs 39, Loss 0.11703268668497913\n",
      "Epochs 40, Loss 0.1099740847130306\n",
      "Epochs 41, Loss 0.1033406625938369\n",
      "Epochs 42, Loss 0.09753791165712755\n",
      "Epochs 43, Loss 0.09241659246617928\n",
      "Epochs 44, Loss 0.08719267770356964\n",
      "Epochs 45, Loss 0.08323738729814067\n",
      "Epochs 46, Loss 0.07835615331714507\n",
      "Epochs 47, Loss 0.07490718300687149\n",
      "Epochs 48, Loss 0.07102039729943499\n",
      "Epochs 49, Loss 0.06731656799092889\n",
      "Epochs 50, Loss 0.06388084645732306\n",
      "Epochs 51, Loss 0.060736477898899466\n",
      "Epochs 52, Loss 0.05734094846411608\n",
      "Epochs 53, Loss 0.05457452734844992\n",
      "Epochs 54, Loss 0.05169253460917389\n",
      "Epochs 55, Loss 0.04937627027538838\n",
      "Epochs 56, Loss 0.04717020680254791\n",
      "Epochs 57, Loss 0.04488066387420986\n",
      "Epochs 58, Loss 0.04280984993238235\n",
      "Epochs 59, Loss 0.04061679880396696\n",
      "Epochs 60, Loss 0.038876715378137305\n",
      "Epochs 61, Loss 0.03709159153368091\n",
      "Epochs 62, Loss 0.035625304546556436\n",
      "Epochs 63, Loss 0.034002708016487304\n",
      "Epochs 64, Loss 0.03237931128387572\n",
      "Epochs 65, Loss 0.030869341928337235\n",
      "Epochs 66, Loss 0.02946897341462318\n",
      "Epochs 67, Loss 0.028245505891391076\n",
      "Epochs 68, Loss 0.02697303942113649\n",
      "Epochs 69, Loss 0.025897309817082714\n",
      "Epochs 70, Loss 0.024728754549869336\n",
      "Epochs 71, Loss 0.0237031309879967\n",
      "Epochs 72, Loss 0.022654265820165165\n",
      "Epochs 73, Loss 0.021603905326628592\n",
      "Epochs 74, Loss 0.020630186299968045\n",
      "Epochs 75, Loss 0.01977550791343674\n",
      "Epochs 76, Loss 0.0188997503646533\n",
      "Epochs 77, Loss 0.018030811384960543\n",
      "Epochs 78, Loss 0.01728341847046977\n",
      "Epochs 79, Loss 0.016552462075196672\n",
      "Epochs 80, Loss 0.015769268036819994\n",
      "Epochs 81, Loss 0.015091583474713843\n",
      "Epochs 82, Loss 0.014360959499754244\n",
      "Epochs 83, Loss 0.013704095530556515\n",
      "Epochs 84, Loss 0.013120629439072218\n",
      "Epochs 85, Loss 0.012473884100472787\n",
      "Epochs 86, Loss 0.011880139969434822\n",
      "Epochs 87, Loss 0.011329023433063412\n",
      "Epochs 88, Loss 0.010782433626445709\n",
      "Epochs 89, Loss 0.010277896748448256\n",
      "Epochs 90, Loss 0.009814906734391116\n",
      "Epochs 91, Loss 0.009230664982169401\n",
      "Epochs 92, Loss 0.008787089289398864\n",
      "Epochs 93, Loss 0.008347806622623466\n",
      "Epochs 94, Loss 0.007990499110746896\n",
      "Epochs 95, Loss 0.0076293746642477345\n",
      "Epochs 96, Loss 0.007311974424737855\n",
      "Epochs 97, Loss 0.006977780698434799\n",
      "Epochs 98, Loss 0.006701019266984076\n",
      "Epochs 99, Loss 0.006411983349607908\n",
      "Epochs 100, Loss 0.006154769280328765\n",
      "Epochs 101, Loss 0.005869789003554615\n",
      "Epochs 102, Loss 0.00562877927768568\n",
      "Epochs 103, Loss 0.005389199313867721\n",
      "Epochs 104, Loss 0.005175126509129768\n",
      "Epochs 105, Loss 0.004933046893711435\n",
      "Epochs 106, Loss 0.00472791097672598\n",
      "Epochs 107, Loss 0.004525995735093602\n",
      "Epochs 108, Loss 0.0043296809071762254\n",
      "Epochs 109, Loss 0.004138967604376376\n",
      "Epochs 110, Loss 0.003976976447120251\n",
      "Epochs 111, Loss 0.003815222385128436\n",
      "Epochs 112, Loss 0.0036376195666889544\n",
      "Epochs 113, Loss 0.003476464838058746\n",
      "Epochs 114, Loss 0.003333424454467604\n",
      "Epochs 115, Loss 0.0031858572056080448\n",
      "Epochs 116, Loss 0.0030533057606589864\n",
      "Epochs 117, Loss 0.0029153922050682013\n",
      "Epochs 118, Loss 0.0027874903853444266\n",
      "Epochs 119, Loss 0.0026713875759014627\n",
      "Epochs 120, Loss 0.0025550482960170484\n",
      "Epochs 121, Loss 0.0024416873857262544\n",
      "Epochs 122, Loss 0.0023367894546026946\n",
      "Epochs 123, Loss 0.002227720291557489\n",
      "Epochs 124, Loss 0.0021335505252864095\n",
      "Epochs 125, Loss 0.0020389029295984074\n",
      "Epochs 126, Loss 0.001939130820119317\n",
      "Epochs 127, Loss 0.0018591452667351405\n",
      "Epochs 128, Loss 0.0017687891290734115\n",
      "Epochs 129, Loss 0.0016941673802648438\n",
      "Epochs 130, Loss 0.0016206182876885578\n",
      "Epochs 131, Loss 0.0015427780726895435\n",
      "Epochs 132, Loss 0.0014794799744777265\n",
      "Epochs 133, Loss 0.0014129637402220396\n",
      "Epochs 134, Loss 0.0013509769883057743\n",
      "Epochs 135, Loss 0.001294711934406223\n",
      "Epochs 136, Loss 0.0012389236535454984\n",
      "Epochs 137, Loss 0.0011857578465424012\n",
      "Epochs 138, Loss 0.0011304464378554258\n",
      "Epochs 139, Loss 0.0010846710911209811\n",
      "Epochs 140, Loss 0.0010394919790996937\n",
      "Epochs 141, Loss 0.0009932400134857744\n",
      "Epochs 142, Loss 0.0009518753422526061\n",
      "Epochs 143, Loss 0.0009099144758693001\n",
      "Epochs 144, Loss 0.0008704569913788873\n",
      "Epochs 145, Loss 0.0008320725271460105\n",
      "Epochs 146, Loss 0.0007942838356029824\n",
      "Epochs 147, Loss 0.0007605482455801393\n",
      "Epochs 148, Loss 0.0007270510352554993\n",
      "Epochs 149, Loss 0.0006956995150630974\n",
      "Epochs 150, Loss 0.0006643479978265532\n",
      "Epochs 151, Loss 0.0006351421843646676\n",
      "Epochs 152, Loss 0.0006118967362453986\n",
      "Epochs 153, Loss 0.0005818564293349482\n",
      "Epochs 154, Loss 0.0005571804108512879\n",
      "Epochs 155, Loss 0.0005331004661002225\n",
      "Epochs 156, Loss 0.0005080668117898313\n",
      "Epochs 157, Loss 0.0004870862292136735\n",
      "Epochs 158, Loss 0.0004634831170733378\n",
      "Epochs 159, Loss 0.00044393297685019206\n",
      "Epochs 160, Loss 0.0004258133797065966\n",
      "Epochs 161, Loss 0.00040805138496580184\n",
      "Epochs 162, Loss 0.00038862045380483323\n",
      "Epochs 163, Loss 0.00037085843007389485\n",
      "Epochs 164, Loss 0.00035464610130020446\n",
      "Epochs 165, Loss 0.000339983492040119\n",
      "Epochs 166, Loss 0.0003248440276593101\n",
      "Epochs 167, Loss 0.00031018141328331694\n",
      "Epochs 168, Loss 0.0002950419551552841\n",
      "Epochs 169, Loss 0.00028180982371850405\n",
      "Epochs 170, Loss 0.000268458481059497\n",
      "Epochs 171, Loss 0.0002560608055546254\n",
      "Epochs 172, Loss 0.00024247103067409626\n",
      "Epochs 173, Loss 0.00023066938376814505\n",
      "Epochs 174, Loss 0.0002208942718198159\n",
      "Epochs 175, Loss 0.0002104039072037267\n",
      "Epochs 176, Loss 0.00019991354838566622\n",
      "Epochs 177, Loss 0.00019168814208114782\n",
      "Epochs 178, Loss 0.0001832243149237911\n",
      "Epochs 179, Loss 0.00017392602853760764\n",
      "Epochs 180, Loss 0.00016665429222939565\n",
      "Epochs 181, Loss 0.00015687915959006205\n",
      "Epochs 182, Loss 0.0001515147605459788\n",
      "Epochs 183, Loss 0.0001453158929507481\n",
      "Epochs 184, Loss 0.0001385209822615252\n",
      "Epochs 185, Loss 0.000131726076006089\n",
      "Epochs 186, Loss 0.00012671930261376474\n",
      "Epochs 187, Loss 0.00012290461421571308\n",
      "Epochs 188, Loss 0.00011754020960097478\n",
      "Epochs 189, Loss 0.00011169896879437147\n",
      "Epochs 190, Loss 0.00010681139963253372\n",
      "Epochs 191, Loss 0.0001020430365770153\n",
      "Epochs 192, Loss 9.644021133681235e-05\n",
      "Epochs 193, Loss 9.15526406402023e-05\n",
      "Epochs 194, Loss 8.773794934313628e-05\n",
      "Epochs 195, Loss 8.416167901259541e-05\n",
      "Epochs 196, Loss 8.02277789944128e-05\n",
      "Epochs 197, Loss 7.545941392095301e-05\n",
      "Epochs 198, Loss 7.283681378567053e-05\n",
      "Epochs 199, Loss 6.949895973207276e-05\n",
      "Epochs 200, Loss 6.711477777088248e-05\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for e in range(epochs):\n",
    "    total_loss=0\n",
    "    for que,ans in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        #forward pass\n",
    "        op=model(que)\n",
    "        #loss\n",
    "        loss=criterion(op,ans[0])\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "    print(f\"Epochs {e+1}, Loss {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "42a2e374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Answer: paris\n",
      "Confidence: 1.00\n"
     ]
    }
   ],
   "source": [
    "def predict(model,que,vocab,threshold=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    num_que=text_to_indices(que,vocab)\n",
    "    que_tensor=torch.tensor(num_que)\n",
    "    que_tensor = que_tensor.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        op = model(que_tensor)\n",
    "    probs=torch.nn.functional.softmax(op,dim=1)\n",
    "    value,index=torch.max(probs,dim=1)\n",
    "\n",
    "    confidence = value.item()\n",
    "    predicted_index = index.item()\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "    if confidence < threshold:\n",
    "        print(\"I don't know\")\n",
    "    else:\n",
    "        # Get the predicted word using the reverse vocabulary\n",
    "        predicted_word = reverse_vocab.get(predicted_index, '<UNK>')\n",
    "        print(f\"Predicted Answer: {predicted_word}\")\n",
    "        print(f\"Confidence: {confidence:.2f}\")\n",
    "predict(model,\"What is the capital of france ?\",vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
